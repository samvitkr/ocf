{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broad-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define paths explicitly here\n",
    "DATA_FILE = '/users/1/kuma0458/open_channel_ret180/data/filter_calc_input.mat'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Running on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "velvet-backing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, is_first=False, omega_0=30):\n",
    "        super().__init__()\n",
    "        self.omega_0 = omega_0\n",
    "        self.is_first = is_first\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return torch.sin(self.omega_0 * self.linear(input))\n",
    "\n",
    "class DualFilterSiren(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            SineLayer(3, 256, is_first=True, omega_0=30),\n",
    "            SineLayer(256, 256, is_first=False, omega_0=30),\n",
    "            SineLayer(256, 256, is_first=False, omega_0=30),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fluid-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Define paths explicitly\n",
    "DATA_DIR = '/users/1/kuma0458/open_channel_ret180/data'\n",
    "DATA_FILENAME = 'filter_calc_input.mat'\n",
    "\n",
    "def load_matlab_data():\n",
    "    filepath = os.path.join(DATA_DIR, DATA_FILENAME)\n",
    "    print(f\"Attempting to load data from: {filepath}\")\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Could not find data file at: {filepath}\")\n",
    "\n",
    "    with h5py.File(filepath, 'r') as f:\n",
    "        # Load and Transpose (Fixing MATLAB v7.3 dimension swap)\n",
    "        if 'phin' not in f.keys():\n",
    "             raise KeyError(\"Variable 'phin' not found.\")\n",
    "        \n",
    "        phin = np.array(f['phin']).transpose()\n",
    "        W    = np.array(f['W']).transpose()\n",
    "        Kxn  = np.array(f['Kxn']).transpose()\n",
    "        Kyn  = np.array(f['Kyn']).transpose()\n",
    "        Zn   = np.array(f['Zn']).transpose()\n",
    "\n",
    "    print(f\"Original Data Shapes: {phin.shape}\")\n",
    "\n",
    "    # Flatten inputs\n",
    "    coords_flat = np.stack([Kxn.flatten(), Kyn.flatten(), Zn.flatten()], axis=1)\n",
    "    targets_flat = (phin.flatten() < 0).astype(np.longlong)\n",
    "    weights_flat = W.flatten()\n",
    "\n",
    "    # --- THE FIX: CLEAN UP NaNs and Infs ---\n",
    "    # 1. Check for NaNs (0/0 errors from normalization)\n",
    "    if np.isnan(coords_flat).any() or np.isnan(weights_flat).any():\n",
    "        print(\"⚠️ Warning: NaNs detected! Replacing with 0.0 to prevent crash.\")\n",
    "        coords_flat = np.nan_to_num(coords_flat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        weights_flat = np.nan_to_num(weights_flat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "    # 2. Clip weights to be safe (strictly between 0 and 1)\n",
    "    weights_flat = np.clip(weights_flat, 0.0, 1.0)\n",
    "    # ----------------------------------------\n",
    "    \n",
    "    return (torch.tensor(coords_flat, dtype=torch.float32),\n",
    "            torch.tensor(targets_flat, dtype=torch.long),\n",
    "            torch.tensor(weights_flat, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-intention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load data from: /users/1/kuma0458/open_channel_ret180/data/filter_calc_input.mat\n",
      "Original Data Shapes: (192, 255, 319)\n",
      "Starting training...\n",
      "Epoch 0 | Loss: 0.004173\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "# ... (Paste load_matlab_data logic here) ...\n",
    "coords, targets, weights = load_matlab_data() # Ensure this function is defined\n",
    "\n",
    "# Create DataLoader\n",
    "# CRITICAL FOR JUPYTER: num_workers=0\n",
    "dataset = TensorDataset(coords, targets, weights)\n",
    "dataloader = DataLoader(dataset, batch_size=32768, shuffle=True, num_workers=0)\n",
    "\n",
    "# Init Model\n",
    "model = DualFilterSiren().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training Loop with Progress Plotting\n",
    "loss_history = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(2000): # Adjust epochs as needed\n",
    "    epoch_loss = 0\n",
    "    for batch_coords, batch_targets, batch_weights in dataloader:\n",
    "        batch_coords, batch_targets, batch_weights = \\\n",
    "            batch_coords.to(DEVICE), batch_targets.to(DEVICE), batch_weights.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_coords)\n",
    "        \n",
    "        # Weighted Loss\n",
    "        raw_loss = torch.nn.functional.cross_entropy(logits, batch_targets, reduction='none')\n",
    "        loss = (raw_loss * batch_weights).mean()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    # Log Average Loss\n",
    "    avg = epoch_loss / len(dataloader)\n",
    "    loss_history.append(avg)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch} | Loss: {avg:.6f}\")\n",
    "\n",
    "# Plot Loss Curve inline\n",
    "plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.yscale('log')\n",
    "plt.title('Training Convergence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-company",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Deep Learning)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
